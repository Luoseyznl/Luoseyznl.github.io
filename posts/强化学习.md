---
title: 强化学习的数学原理
date: 2025-08-04
category: 温故知新
tags: [强化学习, 数学原理, 马尔可夫决策过程]
---

强化学习脱胎于最优控制理论，核心思想是通过与环境的交互来学习最优策略。强化学习的数学基础主要包括马尔可夫决策过程（MDP）、动态规划、梯度下降等理论。本文通过阅读西湖大学赵世钰老师的Mathematical Foundation of Reinforcement Learning，梳理强化学习的核心数学原理。

---

## 目录

- [1. 基本概念（符号定义与问题描述）](#1-基本概念符号定义与问题描述)
- [2. 马尔可夫决策过程（MDP）与贝尔曼方程](#2-马尔可夫决策过程mdp与贝尔曼方程)
  - [2.1 贝尔曼方程](#21-贝尔曼方程)
  - [2.2 贝尔曼最优方程](#22-贝尔曼最优方程)
- [3. 强化学习的核心算法](#3-强化学习的核心算法)
  - [3.1 值迭代与策略迭代](#31-值迭代与策略迭代)
  - [3.2 蒙特卡洛方法](#32-蒙特卡洛方法)
  - [3.3 随机近似 Stochastic Approximation](#33-随机近似-stochastic-approximation)
    - [3.3.1 Robbin-Monro Algorithm](#331-robbin-monro-algorithm)
    - [3.3.2 Dvoretzky's convergence theorem](#332-dvoretzkys-convergence-theorem)

---

# 1. 基本概念（符号定义与问题描述）

强化学习目的是为了解决在给定状态下，如何选择动作以最大化长期奖励的问题。我们可以通过以下四要素描述一个强化学习问题：
- **状态空间**（State Space）：所有可能的状态集合，通常用 $S = \{s_1, s_2, \ldots, s_n\}$ 表示。
- **动作空间**（Action Space）：所有可能的动作集合，通常用 $A = \{a_1, a_2, \ldots, a_m\}$ 表示。
- **状态转移概率**（State Transition Probability）：描述在状态 $s$下采取动作 $a$ 后转移到状态 $s'$ 的概率，通常用 $P(s'|s, a)$ 表示。
- **奖励函数**（Reward Function）：描述在状态 $s$ 下采取动作 $a$ 后获得的即时奖励，通常用 $R(s, a)$ 表示。

现实世界中的决策不能仅考虑当前状态和眼前收益，每一步微小的决策都会对未来的状态及其转移概率造成影响。但在理论分析的世界里，我们可以假设状态转移是已知且仅受当前状态和动作影响（memoryless 无记忆性），奖励函数也是已知的。通过引入**马尔可夫决策过程**（MDP），我们可以将强化学习问题形式化为一个四元组 $(S, A, P, R)$。

在每一个状态 $s$ 下，智能体可以选择一个动作 $a$，然后根据状态转移概率 $P(s'|s, a)$ 转移到下一个状态 $s'$，并获得奖励 $R(s, a)$。智能体的目标是学习一个策略 $\pi$，使得在每个状态下选择的动作能够最大化长期奖励。所谓策略 $\pi$，是指在每个状态下选择动作的概率分布，通常用 $\pi(a|s)$ 表示。

---

# 2. 马尔可夫决策过程（MDP）与贝尔曼方程

上述基本概念成功将强化学习问题形式化为一个马尔可夫决策过程（MDP）。为了进一步分析和求解MDP，我们引入动态规划理论中的**贝尔曼方程**。贝尔曼方程描述了在给定状态下，智能体选择动作后获得的即时奖励和未来状态的价值之间的关系。

首先，我们定义状态值函数 $V(s)$，表示在状态 $s$ 下，智能体能够获得的最大长期奖励（也就是价值函数）。贝尔曼方程可以表示为：
$$
V(s) = \max_{a \in A} \left( R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V(s') \right)
$$
其中 $\gamma$ 是折扣因子，表示未来奖励的衰减程度（$0 \leq \gamma < 1$）。

> $\gamma$ 可以将未来奖励折现到当前时刻，时间相距越远，未来奖励对当前决策的影响越小。

很明显这是一个组合优化问题（问题的最优解可以由其子问题的最优解构成）。我们引入贝尔曼方程（求解状态值函数）和贝尔曼最优方程（求解最优策略），可以通过动态规划方法来求解MDP。

## 2.1 贝尔曼方程

$$
\begin{aligned}
V(s) &= E[R(s, a) + \gamma V(s') | s, a] \\\\
&= \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) \left( R(s, a) + \gamma V(s') \right) \\\\
&= \sum_{a \in A} \pi(a|s) \left( \sum_{s' \in S} P(s'|s, a) R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V(s') \right), \quad \forall s \in S
\end{aligned}
$$

- 策略评估

    所谓策略评估（policy evaluation）就是求解给定策略的状态值函数。很自然想到，通过向量堆叠状态值函数 $V$ 和状态转移概率 $P$，我们可以将贝尔曼方程转化为矩阵形式：

    $$
    V = R + \gamma P V
    $$
    其中 $V = [V(s_1), V(s_2), \ldots, V(s_n)]^T$，$R = [R(s_1), R(s_2), \ldots, R(s_n)]^T$，$P$ 是状态转移概率矩阵。

    易得（但不好求，维度太大）：
    $$
    V = (I - \gamma P)^{-1} R
    $$

    我们引入迭代解法（iterative method）来求解贝尔曼方程。通过迭代更新状态值函数向量 $V$ 直到收敛：

    $$
    V_{k+1} = R + \gamma P V_k
    $$
    此类 bootstrapping 方法（即利用当前估计来改进下一次估计）只要 $\gamma < 1$ 就能保证收敛。

## 2.2 贝尔曼最优方程

强化学习的目的在于与环境交互的过程中获得问题的最优策略。为了解决最优性我们引入**贝尔曼最优方程 Bellman optimal equation (BOE)**。贝尔曼最优方程描述了在给定状态下选择最优动作后获得的最大长期奖励。

$$
V^* (s) = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left( R(s, a) + \gamma V^* (s') \right), \quad \forall s \in S
$$

其矩阵形式与贝尔曼方程类似（理解为考虑最优动作的贝尔曼方程）：
$$
V^* = R + \gamma P V^*
$$

求解贝尔曼最优方程的过程称为**策略迭代**（policy iteration）。策略迭代包括两个步骤：策略评估和策略改进。

- **策略评估**：通过迭代更新状态值函数 $V$ 直到收敛，得到当前策略下的状态值函数。

$$
V_{k+1} = R + \gamma P V_k
$$

- **策略改进**：根据当前状态值函数 $V$ 更新策略 $\pi$，使得在每个状态下选择的动作能够最大化长期奖励。
$$
\pi_{k+1}(s) = \arg\max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left( R(s, a) + \gamma V_k(s') \right), \quad \forall s \in S
$$

赵世钰老师通过 Theorem 3.4 证明了策略迭代算法的最优性。简单理解为：对于任意策略$\pi$，由于 $\gamma < 1$，$V^\* - V_{\pi} \geq \lim_{n \to \infty}\gamma^n (V^\* - V_{\pi}) = 0$。

---

# 3. 强化学习的核心算法

## 3.1 值迭代与策略迭代

值迭代（Value Iteration）和策略迭代（Policy Iteration）是强化学习中两种经典的求解 MDP 的方法。在状态转移概率、奖励函数已知，且状态和动作空间有限的情况下（实际应用中较少见，通常需要通过采样来估计状态转移概率和奖励函数），这两种方法都可以有效地求解最优策略。

值迭代和策略迭代的主要区别在于它们的更新方式：
- **值迭代**：直接更新状态值函数 $V$，直到收敛。每次迭代都计算所有状态的最大值，并更新状态值函数。

    $$
    V_{k+1} = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left( R(s, a) + \gamma V_k(s') \right), \quad \forall s \in S
    $$

    ```
    initialize V(s) for all s in S
    repeat
        for each s in S do
            V_new(s) = max_a [sum_s' P(s'|s, a) * (R(s, a) + gamma * V(s'))]
        end for
        V = V_new
    until convergence
    ```

- **策略迭代**：先评估当前策略的状态值函数 $V$，然后根据状态值函数更新策略。每次迭代都先评估当前策略，然后改进策略。

    $$
    V_{k+1} = R + \gamma P V_k
    $$

    $$
    \pi_{k+1}(s) = \arg\max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left( R(s, a) + \gamma V_k(s') \right), \quad \forall s \in S
    $$

    ```
    initialize V(s) for all s in S
    repeat
        // Policy Evaluation
        for each s in S do
            V_new(s) = R(s, pi(s)) + gamma * sum_s' P(s'|s, pi(s)) * V(s')
        end for
        V = V_new
        // Policy Improvement
        policy_stable = true
        for each s in S do
            old_action = pi(s)
            pi(s) = argmax_a [sum_s' P(s'|s, a) * (R(s, a) + gamma * V(s'))]
            if old_action != pi(s) then
                policy_stable = false
            end if
        end for
    until policy_stable
    ```

## 3.2 蒙特卡洛方法

蒙特卡洛方法（Monte Carlo Method）是一种基于随机采样的强化学习方法。通过与环境的交互，收集状态、动作和奖励的样本，然后利用这些样本来估计状态值函数或动作值函数。蒙特卡洛方法不需要知道状态转移概率和奖励函数，可以直接从环境中采样。

> 大数定理：只要采样数量足够大，样本均值就会收敛到真实的期望值。样本方差也会随之减小。
> $$
> \begin{aligned} 
> E[\bar{x}] &= E[X] \\\\
> var(\bar{x}) &= \frac{1}{n^2} \sum_{i=1}^{n} var(X_i) = \frac{1}{n^2} n \cdot var(X) = \frac{var(X)}{n}
> \end{aligned}
> $$

与策略迭代中动作价值 $Q_{\pi_k}(s, a) = E[R(s, a) + \gamma V_{\pi_k}(s')]$ 类似，蒙特卡洛方法中的动作价值可以表示为：
$$
\begin{aligned}
Q_{\pi_k}(s, a) &= E[R(s, a) + \gamma V_{\pi_k}(s')] \\\\
                &\approx \frac{1}{N(s, a)} \sum_{i=1}^{N(s, a)} R_i(s, a)
\end{aligned}
$$

```
Initialize: 给定初始策略 PI

Repeat until策略收敛:
    For每个状态 s in S:
        For每个动作 a in A(s):
            采集足够多的以 (s, a) 开始、按当前策略 PI_k 运行的完整轨迹（episode）
            计算所有这些轨迹的平均回报，作为 Q_k(s, a) 的估计

    For每个状态 s in S:
        策略改进：选择使 Q_k(s, a) 最大的动作 a*_k(s)
        更新策略为确定性策略：
            PI_k+1(a|s) = 1 if a = a*_k(s)
            PI_k+1(a|s) = 0 otherwise
```

我们称有概率选择所有动作的策略为**ε-贪婪策略**（ε-greedy policy），即在每个状态下以概率 ε 选择随机动作，以概率 1-ε 选择当前最优动作。这样可以保证探索和利用之间的平衡。因此在采用了ε-贪婪策略的情况下，蒙特卡洛方法可以通过一个无限长的随机过程来收敛到最优策略。

## 3.3 随机近似 Stochastic Approximation

随机近似（Stochastic Approximation）是一类用于在噪声或采样不确定的情况下逼近最优解的迭代方法。比如统计学中的样本均值 $E[\bar{x}] = \frac{1}{n} \sum_{i=1}^{n} x_i$，很容易构造 $w_k = \frac{1}{k} \sum_{i=1}^{k} x_i$，进而得到
$$
\begin{aligned}
w_{k+1} &= w_k + \frac{1}{k+1} (x_{k+1} - w_k) \\\\
        &= w_k + \alpha_k (x_{k+1} - w_k)
\end{aligned}
$$

### 3.3.1 Robbin-Monro Algorithm

经典的随机近似算法，Robbin-Monro 算法通过迭代更新来逼近最优解。针对方程：
$$
g(w) = 0
$$

当 $g(w)$ 函数形式未知，但可通过采样得到 $g(w_k)$ 的值，Robbin-Monro 算法可通过迭代更新以逼近最优解 $w^*$：

$$
\begin{aligned}
\tilde{g}(w, \eta) & = g(w) + \eta \\\\
w_{k+1} & = w_k - \alpha_k \tilde{g}(w_k, \eta_k)
\end{aligned}
$$

我们可以进一步总结出 Robbins-Monro Theorem：

如果满足：1）$0 \leq c_1 \leq \nabla g(w) \leq c_2 < \infty$，2）$\sum_{k=1}^{\infty} \alpha_k = \infty$，3）$\sum_{k=1}^{\infty} \alpha_k^2 < \infty$，3）$E[\eta_k|w_k]=0, E[\eta_k^2|w_k] \leq C$，则 $w_k$ 收敛到 $w^*$。


上述三个条件可以解读为只要g(w)满足一定的光滑性（smoothness）和采样噪声（noise）无偏性（unbiasedness），学习率 $\alpha_k$ 逐渐减小（但减少速度不过快），Robbins-Monro 算法就能收敛到最优解。

### 3.3.2 Dvoretzky's convergence theorem

Robbins-Monro 算法是点态（pointwise）随机近似方法的开端，它关注在单点 $w_k$ 上估计 $g(w_k)$，并通过代找到零点 $w^*$。而 Dvoretzky's convergence theorem 关注的是：**随机过程（如经验分布函数）如何在整个定义域上一致地逼近目标函数**。

针对随机过程 $w_{k+1} = (1-\alpha_k) w_k + \beta_k \eta_k$，Dvoretzky 提出了以下定理：

如果满足：1）$0 \leq c_1 \leq \nabla g(w) \leq c_2 < \infty$，2）$\sum_{k=1}^{\infty} \alpha_k = \infty$，3）$\sum_{k=1}^{\infty} \alpha_k^2 < \infty$，3）$E[\eta_k|w_k]=0, E[\eta_k^2|w_k] \leq C$，则 $w_k$ 收敛到 $w^*$。
