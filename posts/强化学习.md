---
title: 强化学习的数学原理
date: 2025-08-04
category: 温故知新
tags: [强化学习, 数学原理, 马尔可夫决策过程]
---

强化学习脱胎于最优控制理论，核心思想是通过与环境的交互来学习最优策略。强化学习的数学基础主要包括马尔可夫决策过程（MDP）、动态规划、梯度下降等理论。本文通过阅读西湖大学赵世钰老师的Mathematical Foundation of Reinforcement Learning，梳理强化学习的核心数学原理。

---

## 目录

- [1. 基本概念（符号定义与问题描述）](#1-基本概念符号定义与问题描述)
- [2. 马尔可夫决策过程（MDP）与贝尔曼方程](#2-马尔可夫决策过程mdp与贝尔曼方程)
  - [2.1 贝尔曼方程](#21-贝尔曼方程)
  - [2.2 贝尔曼最优方程](#22-贝尔曼最优方程)
- [3. 强化学习的核心算法](#3-强化学习的核心算法)
  - [3.1 值迭代与策略迭代](#31-值迭代与策略迭代)
  - [3.2 蒙特卡洛方法](#32-蒙特卡洛方法)

---

# 1. 基本概念（符号定义与问题描述）

强化学习目的是为了解决在给定状态下，如何选择动作以最大化长期奖励的问题。我们可以通过以下四要素描述一个强化学习问题：
- **状态空间**（State Space）：所有可能的状态集合，通常用 $S = \{s_1, s_2, \ldots, s_n\}$ 表示。
- **动作空间**（Action Space）：所有可能的动作集合，通常用 $A = \{a_1, a_2, \ldots, a_m\}$ 表示。
- **状态转移概率**（State Transition Probability）：描述在状态 $s$下采取动作 $a$ 后转移到状态 $s'$ 的概率，通常用 $P(s'|s, a)$ 表示。
- **奖励函数**（Reward Function）：描述在状态 $s$ 下采取动作 $a$ 后获得的即时奖励，通常用 $R(s, a)$ 表示。

现实世界中的决策不能仅考虑当前状态和眼前收益，每一步微小的决策都会对未来的状态及其转移概率造成影响。但在理论分析的世界里，我们可以假设状态转移是已知且仅受当前状态和动作影响（memoryless 无记忆性），奖励函数也是已知的。通过引入**马尔可夫决策过程**（MDP），我们可以将强化学习问题形式化为一个四元组 $(S, A, P, R)$。

在每一个状态 $s$ 下，智能体可以选择一个动作 $a$，然后根据状态转移概率 $P(s'|s, a)$ 转移到下一个状态 $s'$，并获得奖励 $R(s, a)$。智能体的目标是学习一个策略 $\pi$，使得在每个状态下选择的动作能够最大化长期奖励。所谓策略 $\pi$，是指在每个状态下选择动作的概率分布，通常用 $\pi(a|s)$ 表示。

# 2. 马尔可夫决策过程（MDP）与贝尔曼方程

上述基本概念成功将强化学习问题形式化为一个马尔可夫决策过程（MDP）。为了进一步分析和求解MDP，我们引入动态规划理论中的**贝尔曼方程**。贝尔曼方程描述了在给定状态下，智能体选择动作后获得的即时奖励和未来状态的价值之间的关系。

首先，我们定义状态值函数 $V(s)$，表示在状态 $s$ 下，智能体能够获得的最大长期奖励（也就是价值函数）。贝尔曼方程可以表示为：
$$
V(s) = \max_{a \in A} \left( R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V(s') \right)
$$
其中 $\gamma$ 是折扣因子，表示未来奖励的衰减程度（$0 \leq \gamma < 1$）。

> $\gamma$ 可以将未来奖励折现到当前时刻，时间相距越远，未来奖励对当前决策的影响越小。

很明显这是一个组合优化问题（问题的最优解可以由其子问题的最优解构成）。我们引入贝尔曼方程（求解状态值函数）和贝尔曼最优方程（求解最优策略），可以通过动态规划方法来求解MDP。

## 2.1 贝尔曼方程

$$
\begin{aligned}
V(s) &= E[R(s, a) + \gamma V(s') | s, a] \\\\
&= \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) \left( R(s, a) + \gamma V(s') \right) \\\\
&= \sum_{a \in A} \pi(a|s) \left( \sum_{s' \in S} P(s'|s, a) R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V(s') \right), \quad \forall s \in S
\end{aligned}
$$

- 策略评估

    所谓策略评估（policy evaluation）就是求解给定策略的状态值函数。很自然想到，通过向量堆叠状态值函数 $V$ 和状态转移概率 $P$，我们可以将贝尔曼方程转化为矩阵形式：

    $$
    V = R + \gamma P V
    $$
    其中 $V = [V(s_1), V(s_2), \ldots, V(s_n)]^T$，$R = [R(s_1), R(s_2), \ldots, R(s_n)]^T$，$P$ 是状态转移概率矩阵。

    易得（但不好求，维度太大）：
    $$
    V = (I - \gamma P)^{-1} R
    $$

    我们引入迭代解法（iterative method）来求解贝尔曼方程。通过迭代更新状态值函数向量 $V$ 直到收敛：

    $$
    V_{k+1} = R + \gamma P V_k
    $$
    此类 bootstrapping 方法（即利用当前估计来改进下一次估计）只要 $\gamma < 1$ 就能保证收敛。

## 2.2 贝尔曼最优方程

强化学习的目的在于与环境交互的过程中获得问题的最优策略。为了解决最优性我们引入**贝尔曼最优方程 Bellman optimal equation (BOE)**。贝尔曼最优方程描述了在给定状态下选择最优动作后获得的最大长期奖励。

$$
V^* (s) = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left( R(s, a) + \gamma V^* (s') \right), \quad \forall s \in S
$$

其矩阵形式与贝尔曼方程类似（理解为考虑最优动作的贝尔曼方程）：
$$
V^* = R + \gamma P V^*
$$

求解贝尔曼最优方程的过程称为**策略迭代**（policy iteration）。策略迭代包括两个步骤：策略评估和策略改进。

- **策略评估**：通过迭代更新状态值函数 $V$ 直到收敛，得到当前策略下的状态值函数。

$$
V_{k+1} = R + \gamma P V_k
$$

- **策略改进**：根据当前状态值函数 $V$ 更新策略 $\pi$，使得在每个状态下选择的动作能够最大化长期奖励。
$$
\pi_{k+1}(s) = \arg\max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left( R(s, a) + \gamma V_k(s') \right), \quad \forall s \in S
$$

赵世钰老师通过 Theorem 3.4 证明了策略迭代算法的最优性。简单理解为：对于任意策略$\pi$，由于 $\gamma < 1$，$V^\* - V_{\pi} \geq \lim_{n \to \infty}\gamma^n (V^\* - V_{\pi}) = 0$。

# 3. 强化学习的核心算法

## 3.1 值迭代与策略迭代

值迭代（Value Iteration）和策略迭代（Policy Iteration）是强化学习中两种经典的求解 MDP 的方法。在状态转移概率、奖励函数已知，且状态和动作空间有限的情况下（实际应用中较少见，通常需要通过采样来估计状态转移概率和奖励函数），这两种方法都可以有效地求解最优策略。

值迭代和策略迭代的主要区别在于它们的更新方式：
- **值迭代**：直接更新状态值函数 $V$，直到收敛。每次迭代都计算所有状态的最大值，并更新状态值函数。

    $$
    V_{k+1} = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left( R(s, a) + \gamma V_k(s') \right), \quad \forall s \in S
    $$

    ```
    initialize V(s) for all s in S
    repeat
        for each s in S do
            V_new(s) = max_a [sum_s' P(s'|s, a) * (R(s, a) + gamma * V(s'))]
        end for
        V = V_new
    until convergence
    ```

- **策略迭代**：先评估当前策略的状态值函数 $V$，然后根据状态值函数更新策略。每次迭代都先评估当前策略，然后改进策略。

    $$
    V_{k+1} = R + \gamma P V_k
    $$

    $$
    \pi_{k+1}(s) = \arg\max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left( R(s, a) + \gamma V_k(s') \right), \quad \forall s \in S
    $$

    ```
    initialize V(s) for all s in S
    repeat
        // Policy Evaluation
        for each s in S do
            V_new(s) = R(s, pi(s)) + gamma * sum_s' P(s'|s, pi(s)) * V(s')
        end for
        V = V_new
        // Policy Improvement
        policy_stable = true
        for each s in S do
            old_action = pi(s)
            pi(s) = argmax_a [sum_s' P(s'|s, a) * (R(s, a) + gamma * V(s'))]
            if old_action != pi(s) then
                policy_stable = false
            end if
        end for
    until policy_stable
    ```

## 3.2 蒙特卡洛方法

蒙特卡洛方法（Monte Carlo Method）是一种基于随机采样的强化学习方法。通过与环境的交互，收集状态-动作对的奖励信息，根据这些信息来更新状态值函数或动作值函数。
