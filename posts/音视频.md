---
title: 音视频学习笔记
date: 2025-08-01
category: 渐入佳境
tags: [音视频, 媒体, 开发]
---

## 目录

## 1. 音视频与流媒体基础

一个完整的音视频文件通常由音频流、视频流以及基础元信息（如时长、分辨率、编码格式等）组成。常见的 mp4、mov、flv、avi、rmvb 等格式属于封装格式（容器格式），它们可以将音频、视频和元信息打包在同一个文件中。封装格式内部的音视频数据一般经过压缩处理，常用的视频编码格式有 H.264、Xvid 等，音频编码格式有 AAC、MP3 等。解码后的视频原始数据通常为 YUV 格式，音频原始数据通常为 PCM（脉冲编码调制）格式。

### 1.1 封装格式（容器格式）

目前常见的音视频封装格式有 AVI(.avi)、ASF(.asf)、WMV(.wmv)、QuickTime(.mov)、MP4(.mp4)、MPEG-TS(.ts)、FLV(.flv)、MKV(.mkv) 等。封装格式的主要作用是将音频流、视频流和元信息（如时长、分辨率、编码格式等）组织和打包在同一个文件中。

在封装格式头部，通常会包含视频的码率（Bitrate）、帧率（Frame Rate）和分辨率（Resolution），音频的采样率（Sample Rate）、声道数（Channels）等信息。

- **码率（Bitrate）**：指单位时间内传输或存储的数据量，通常以 kbps（千比特每秒）为单位。码率越高，音视频质量越好，但文件体积也越大。
- **采样率（Sample Rate）**：主要用于音频，表示每秒采集的样本数，单位为 Hz。采样率越高，音频还原度越高，细节越丰富。

> 码率越大，压缩后数据越接近原始数据（保留更多细节），但文件体积也越大。采样率越高，音频质量越好，但也会增加数据量。

- **帧率（Frame Rate）**：指每秒显示的帧数，通常以 FPS（Frames Per Second）为单位。常见的帧率有 24 FPS、30 FPS、60 FPS 等。帧率越高，视频画面越流畅，但数据量也会增加。
- **分辨率（Resolution）**：指视频画面的像素尺寸，通常以宽度 x 高度 表示，如 1920x1080（全高清）。分辨率越高，画面细节越丰富，但数据量也会增加。
- **声道数（Channels）**：指音频的声道数量，常见的有单声道（Mono）、立体声（Stereo）和环绕声（Surround Sound）。声道数越多，音频效果越丰富，但数据量也会增加。

> 帧率和分辨率是视频的关键参数，影响画面流畅度和清晰度。声道数则影响音频的空间感和立体感。

```
视频大小 = 码率 × 时长
音频大小 = 采样率 × 声道数 × 位深度 × 时长
一帧大小 = 分辨率 × 色深
```

### 1.2 编码格式（压缩格式）

编码有硬件编码和软件编码。硬件编码通常依赖于专用的硬件加速器，如 GPU，而软件编码则使用 CPU 进行处理。常见的编码格式有 H.264、H.265、MPEG-4、VP9 等。编码也分为有损和无损两种，有损编码比如 H.264、H.265 等；无损编码则保留所有原始数据，常用于专业领域。

- **H.264**：广泛使用的视频编码格式，具有较高的压缩效率和良好的画质，适用于大多数场景。
  1. **帧间压缩**：利用视频帧之间的冗余信息，只存储帧与帧之间的差异，显著减少数据量。
     - **I 帧（Intra Frame）**：关键帧，包含完整图像信息，通常用于视频的起始帧或场景切换处。
     - **P 帧（Predictive Frame）**：预测帧，仅包含与前一帧的差异信息，依赖于前面的 I 帧 或 P 帧。
     - **B 帧（Bi-directional Frame）**：双向预测帧，依赖于前后两帧的信息，压缩效率更高。
  2. **运动估计与补偿**：分析视频中物体的运动，将相似区域用运动矢量描述，进一步压缩数据。
  3. **变换与量化**：将图像数据转换到频域并量化，去除人眼不敏感的高频信息。
  4. **熵编码**：对剩余数据进行无损压缩，提高编码效率。

> 除了 I 帧、P 帧、B 帧，H.264 还引入了 IDR 帧（Instantaneous Decoder Refresh Frame），允许解码器在此帧处进行随机访问，通常用于流媒体传输。GOP（Group of Pictures）是指一组连续的帧，通常以 I 帧 开头，后面跟随若干 P 帧 和 B 帧。PTS（Presentation Time Stamp）和 DTS（Decoding Time Stamp）用于标记帧的显示和解码时间，确保视频流的正确播放顺序。

### 1.3 解码与原始数据处理

解码也可分为硬件解码和软件解码。硬件解码通常使用 GPU 或专用芯片进行处理，速度更快，功耗更低；软件解码则使用 CPU 进行处理，灵活性更高，但性能较低。YUV 是一种常见的视频原始数据格式，分为 Y（亮度）、U（蓝色差异）和 V（红色差异）三个分量。通过两种色差分量和亮度分量的组合代替 RGB 三个分量，可以减少数据量并保留人眼对亮度的敏感度。PCM（脉冲编码调制）是音频的原始数据格式，通常以采样率和位深度来表示。PCM 数据是未压缩的音频数据，直接反映了声音波形。

- 流媒体传输协议
  - **RTP, RTCP**：实时传输协议，常用于音视频流的传输，支持实时性要求高的应用。
  - **RTSP**：实时流协议，用于控制流媒体服务器，支持播放、暂停、快进等操作。
  - **HTTP Live Streaming (HLS)**：基于 HTTP 的流媒体传输协议，将音视频分割成小片段，适用于不稳定网络环境。
  - **Dynamic Adaptive Streaming over HTTP (DASH)**：动态自适应流媒体传输协议，根据网络状况动态调整视频质量，支持多种编码格式。

- 弱网优化
  - **自适应码率流（Adaptive Bitrate Streaming）**：根据网络状况动态调整视频质量，常用于 HLS 和 DASH。
  - **分段传输**：将音视频文件分割成小片段，按需加载，减少初始加载时间和带宽消耗。
  - **缓冲策略**：通过设置合理的缓冲区大小和预加载策略，平衡流畅度和延迟。


## 2. FFmpeg 入门与编解码

FFmpeg 是一个强大的命令行音视频工具，可以承担转码、混流、嵌挂字幕、剪辑等工作。

```sh
# 安装 FFmpeg 开发库：编解码、封装/解封、通用工具、图像处理、音频处理、设备处理
sudo apt install libavcodec-dev libavformat-dev libavutil-dev libswscale-dev libavdevice-dev libswresample-dev
```

### 2.1 CMakeLists.txt 示例

头文件：`/usr/include/ffmpeg/`
库文件：`/usr/lib/x86_64-linux-gnu/`


```cmake
cmake_minimum_required(VERSION 3.12)
project(video_player VERSION 1.0)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

find_package(PkgConfig REQUIRED)
pkg_check_modules(FFMPEG REQUIRED IMPORTED_TARGET   # 使用 pkg-config 查找 FFmpeg 库
    libavcodec      # 音视频编解码库
    libavformat     # 封装格式处理库
    libavutil       # 通用工具库
    libswscale      # 图像缩放和格式转换库
    libavdevice     # 设备处理库
    libswresample   # 音频重采样库
)

find_package(glfw3 REQUIRED)    # GLFW 库用于窗口和输入管理

add_executable(entry main.cpp)
target_link_libraries(entry PRIVATE
    PkgConfig::FFMPEG           # 依赖 FFmpeg 库
    glfw                        # 依赖 GLFW 库
)
```

### 2.2 FFmpeg 常用命令行工具

- `ffmpeg`：用于音视频转码、剪辑、合并等操作。
  ```sh
  ffmpeg -i input.mp4 -c:v libx264 -c:a aac output.mp4 # 转码为 H.264 编码
  ffmpeg -i input.mp4 -vf "scale=1280:720" output.mp4 # 缩放视频到 1280x720 分辨率
  ffmpeg -i input.mp4 -ss 00:00:10 -to 00:00:20 -c copy output.mp4 # 截取视频
  ffmpeg -i input.mp4 -vf "drawtext=text='hello':fontcolor=white:fontsize=24" output.mp4 # 添加文本水印
  ffmpeg -i input.mp4 -vf "subtitles=subtitle.srt" output.mp4 # 添加字幕
  ffmpeg -i input.mp4 -c copy -f segment -segment_time 10 output%03d.mp4 # 分割视频
  ffmpeg -i input.mp4 -c copy -f flv rtmp://live.example.com/stream # 推流到 RTMP 服务器
  ffmpeg -i input.mp4 -vn -acodec copy output.mp3 # 提取音频流
  ffmpeg -i input.mp4 -an -vcodec copy output.mp4 # 提取视频流
  ```

### 2.3 FFmpeg API：解封装 → 解码 → 数据获取

```c
// 解封装
avformat_open_input(&fmt_ctx, "input.mp4", NULL, NULL);
avformat_find_stream_info(fmt_ctx, NULL);

// 解码
AVCodec *codec = avcodec_find_decoder(AV_CODEC_ID_H264);
AVCodecContext *codec_ctx = avcodec_alloc_context3(codec);
avcodec_open2(codec_ctx, codec, NULL);

// 数据获取
AVPacket *pkt = av_packet_alloc();
AVFrame *frame = av_frame_alloc();
while (av_read_frame(fmt_ctx, pkt) >= 0) {
    avcodec_send_packet(codec_ctx, pkt);
    avcodec_receive_frame(codec_ctx, frame);
    // 处理解码后的数据
}
```

### 2.4 将解码数据保存为 PCM / YUV

```c
// 保存为 PCM
FILE *pcm_file = fopen("output.pcm", "wb");
fwrite(frame->data[0], 1, frame->linesize[0], pcm_file);
fclose(pcm_file);

// 保存为 YUV
FILE *yuv_file = fopen("output.yuv", "wb");
fwrite(frame->data[0], 1, frame->linesize[0], yuv_file);
fwrite(frame->data[1], 1, frame->linesize[1], yuv_file);
fwrite(frame->data[2], 1, frame->linesize[2], yuv_file);
fclose(yuv_file);
```

## 3. 渲染与播放基础

音视频的渲染与播放通常涉及到图形库（如 OpenGL、DirectX）和音频库（如 SDL2、OpenAL）。常见的渲染流程包括创建窗口、加载纹理、编写着色器、处理输入等。

### 3.1 OpenGL 基础（纹理、着色器、顶点）

OpenGL 是一个跨平台的图形渲染 API，常用于 2D/3D 图形渲染。音视频播放器通常使用 OpenGL 来渲染视频帧。

```c
// 初始化 OpenGL
glfwInit();
GLFWwindow *window = glfwCreateWindow(800, 600, "Video Player", NULL, NULL);
glfwMakeContextCurrent(window);
gladLoadGLLoader((GLADloadproc)glfwGetProcAddress);
// 创建纹理
GLuint texture;
glGenTextures(1, &texture);
glBindTexture(GL_TEXTURE_2D, texture);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
// 加载视频帧数据到纹理
glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, GL_RGBA, GL_UNSIGNED_BYTE, frame->data[0]);
// 渲染视频帧
glUseProgram(shader_program);
glBindTexture(GL_TEXTURE_2D, texture);
glDrawArrays(GL_TRIANGLES, 0, 6); // 绘制一个矩形覆盖整个窗口
```

### 3.2 GLFW：窗口与输入管理
GLFW 是一个跨平台的窗口和输入管理库，常用于创建 OpenGL 窗口和处理输入事件。

```c
// 初始化 GLFW
glfwInit();
GLFWwindow *window = glfwCreateWindow(800, 600, "Video Player", NULL, NULL);
glfwMakeContextCurrent(window);
// 设置窗口回调函数
glfwSetKeyCallback(window, key_callback);
// 主循环
while (!glfwWindowShouldClose(window)) {
    glClear(GL_COLOR_BUFFER_BIT);
    // 渲染视频帧
    render_video_frame();
    glfwSwapBuffers(window);
    glfwPollEvents();
}
glfwDestroyWindow(window);
glfwTerminate();
```

### 3.3 SDL2 或其他库：音频播放与音视频同步

SDL2 是一个跨平台的多媒体库，提供了音频播放、视频渲染、输入处理等功能。音视频同步通常通过时间戳（PTS）来实现。

```c
// 初始化 SDL2
SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO);
SDL_Window *window = SDL_CreateWindow("Video Player", SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED, 800, 600, SDL_WINDOW_SHOWN);
SDL_Renderer *renderer = SDL_CreateRenderer(window, -1, SDL_RENDERER_ACCELERATED);
// 初始化音频
SDL_AudioSpec audio_spec;
audio_spec.freq = 44100;
audio_spec.format = AUDIO_S16SYS;
audio_spec.channels = 2;
audio_spec.samples = 1024;
SDL_OpenAudio(&audio_spec, NULL);
// 播放音频
SDL_PauseAudio(0);
// 音视频同步
Uint32 video_pts = 0; // 视频时间戳
Uint32 audio_pts = 0; // 音频时间戳
while (running) {
    // 获取当前时间戳
    Uint32 current_time = SDL_GetTicks();
    // 计算音视频同步
    if (video_pts < audio_pts) {
        SDL_Delay(audio_pts - video_pts); // 等待音频播放
    } else {
        SDL_Delay(video_pts - audio_pts); // 等待视频播放
    }
    // 渲染视频帧
    render_video_frame(renderer);
    // 播放音频帧
    play_audio_frame();
    // 更新时间戳
    video_pts += frame_duration; // 假设每帧持续时间为 frame_duration
    audio_pts += audio_frame_duration; // 假设每音频帧持续时间为 audio_frame_duration
}
SDL_DestroyRenderer(renderer);
SDL_DestroyWindow(window);
SDL_Quit();
```

## 4. 播放器架构设计
### 4.1 模块划分（解码线程、音频线程、视频线程）
### 4.2 缓冲与同步机制
### 4.3 异常与性能优化
