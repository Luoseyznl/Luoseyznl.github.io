---
title: 音视频学习笔记
date: 2025-08-01
category: 渐入佳境
tags: [音视频, 媒体, 开发]
---

## 目录

## 1. 音视频与流媒体基础

一个完整的音视频文件通常由多个流（如音频流、视频流、字幕流等）和基础元信息组成。常见的封装格式（容器格式）如 mp4、mov、flv、avi、rmvb 等，可以将不同类型的流和元信息打包在同一个文件中。封装格式内部的音视频数据一般经过压缩处理，常用的视频编码格式有 H.264、Xvid 等，音频编码格式有 AAC、MP3 等。解码后的视频原始数据通常为 YUV 格式，音频原始数据通常为 PCM（脉冲编码调制）格式。

### 1.1 封装格式（容器格式）

目前常见的音视频封装格式有 AVI(.avi)、ASF(.asf)、WMV(.wmv)、QuickTime(.mov)、MP4(.mp4)、MPEG-TS(.ts)、FLV(.flv)、MKV(.mkv) 等。封装格式的主要作用是将多个流（音频流、视频流、字幕流等）和元信息组织和打包在同一个文件中。

全局元信息（Global Metadata）
- 文件标题
- 作者
- 创建时间
- 编码器信息
- 封面图片
- 版权信息
- 章节信息

流级元信息（Stream Metadata）
- 视频流：分辨率、帧率、码率、编码格式、时长等
  - **码率（Bitrate）**：单位时间内传输或存储的数据量，通常以 kbps（千比特每秒）为单位。
  - **帧率（Frame Rate）**：每秒显示的帧数，单位为 FPS。
  - **分辨率（Resolution）**：视频画面的像素尺寸，如 1920x1080。
  - **色深（Color Depth）**：每个像素的比特数。
  - **编码格式**：如 H.264、H.265 等，决定压缩方式和兼容性。
  - **时长（Duration）**：视频流的总播放时间。
- 音频流：采样率、声道数、位深度、码率、编码格式、时长等
  - **码率（Bitrate）**：单位时间内传输或存储的数据量，通常以 kbps（千比特每秒）为单位。
  - **采样率（Sample Rate）**：主要用于音频，表示每秒采集的样本数，单位为 Hz。采样率越高。
  - **声道数（Channels）**：指音频的声道数量，常见的有单声道（Mono）、立体声（Stereo）和环绕声（Surround Sound）。
  - **位深（Bit Depth）**：每个样本的比特数，常见的有 16 位、24 位等。
  - **编码格式**：如 AAC、MP3 等，决定压缩方式和兼容性。
  - **时长（Duration）**：音频流的总播放时间。
- 字幕流：语言、格式等

> 视频大小(KB) =  码率（kbps） × 时长（秒） ÷ 8 = 分辨率（像素） × 色深（bit） ÷ 8 × 帧率（FPS） × 时长（秒）
> 音频大小(KB) =  码率（kbps） × 时长（秒） ÷ 8 = 采样率（Hz） × 位深（bit） × 声道数 ÷ 8 × 时长（秒）
> 一帧大小(KB) = 分辨率（像素） × 色深（bit） ÷ 8

```sh
# 查看 trump.mp4 文件的所有流信息
ffmpeg -i trump.mp4

# 只显示流信息（更详细）
ffprobe trump.mp4

# 只显示流类型和编号
ffprobe -show_streams trump.mp4
```


### 1.2 编码格式（压缩格式）

编码有硬件编码和软件编码。硬件编码通常依赖于专用的硬件加速器，如 GPU，而软件编码则使用 CPU 进行处理。常见的编码格式有 H.264、H.265、MPEG-4、VP9 等。编码也分为有损和无损两种，有损编码比如 H.264、H.265 等，去除人眼/耳不敏感的信息，牺牲部分质量换取更高压缩率；无损编码则保留所有原始数据，常用于专业领域。

- **H.264**：广泛使用的视频编码格式，具有较高的压缩效率和良好的画质，适用于大多数场景。
  1. **帧间压缩**：利用视频帧之间的冗余信息，只存储帧与帧之间的差异，显著减少数据量。
     - **I 帧（Intra Frame）**：关键帧，包含完整图像信息，通常用于视频的起始帧或场景切换处。
     - **P 帧（Predictive Frame）**：预测帧，仅包含与前一帧的差异信息，依赖于前面的 I 帧 或 P 帧。
     - **B 帧（Bi-directional Frame）**：双向预测帧，依赖于前后两帧的信息，压缩效率更高。
  2. **运动估计与补偿**：分析视频中物体的运动，将相似区域用运动矢量描述，进一步压缩数据。
  3. **变换与量化**：将图像数据转换到频域并量化，去除人眼不敏感的高频信息。
  4. **熵编码**：对剩余数据进行无损压缩，提高编码效率。

> 除了 I 帧、P 帧、B 帧，H.264 还引入了 IDR 帧（Instantaneous Decoder Refresh Frame），允许解码器在此帧处进行随机访问，通常用于流媒体传输。GOP（Group of Pictures）是指一组连续的帧，通常以 I 帧 开头，后面跟随若干 P 帧 和 B 帧。PTS（Presentation Time Stamp）和 DTS（Decoding Time Stamp）用于标记帧的显示和解码时间，确保视频流的正确播放顺序。

### 1.3 解码与原始数据处理

解码也可分为硬件解码和软件解码。硬件解码通常使用 GPU 或专用芯片进行处理，速度更快，功耗更低；软件解码则使用 CPU 进行处理，灵活性更高，但性能较低。YUV 是一种常见的视频原始数据格式，分为 Y（亮度）、U（蓝色差异）和 V（红色差异）三个分量。由于人眼对亮度敏感而对色差不敏感，YUV可以按照 4:2:0、4:2:2、4:4:4 等不同的子采样方式进行编码（压缩色度分量）。PCM（脉冲编码调制）是未压缩的音频的原始数据格式，通常以采样率和位深度来表示。

#### 流媒体传输协议

- **RTP/RTCP（Real-time Transport Protocol/Control Protocol）**  
  RTP 用于实时传输音视频数据，RTCP 用于监控传输质量和同步。常用于视频会议、直播等对实时性要求高的场景。
- **RTSP（Real Time Streaming Protocol）**  
  用于控制流媒体服务器，支持播放、暂停、快进等操作。RTSP 本身不传输数据，通常与 RTP 配合使用。
- **HLS（HTTP Live Streaming）**  
  苹果提出的流媒体协议，将音视频切分为小片段（TS 文件），通过 HTTP 传输，客户端根据网络状况选择合适码率的片段，适用于不稳定网络环境。
- **DASH（Dynamic Adaptive Streaming over HTTP）**  
  类似 HLS，支持多种编码格式和自适应码率，客户端根据网络状况动态切换视频质量，提升观看体验。

#### 弱网优化

- **自适应码率流（Adaptive Bitrate Streaming）**  
  根据用户当前网络状况自动切换不同码率的视频流，保证流畅播放，常用于 HLS 和 DASH。
- **分段传输**  
  将音视频文件分割成多个小片段，客户端按需加载，减少初始加载时间和带宽消耗，提高抗网络波动能力。
- **缓冲策略**  
  设置合理的缓冲区大小和预加载策略，平衡播放流畅度和延迟，避免卡顿和中断。


## 2. FFmpeg

FFmpeg 是一套完整的音视频处理工具链（CLI + libav* 系列库），提供完整的音视频编解码、转码、封装、解复用等功能。FFmpeg 支持几乎所有的音视频格式和编解码器，广泛应用于音视频处理、流媒体传输等领域。
整个调用链大致是：设备/文件 → 封装层 → 解码层 → （视频走 swscale，音频走 swresample）→ 渲染/播放

### 2.1 FFmpeg 主要库介绍

- **libavdevice**：采集设备接口（如摄像头、麦克风）
- **libavformat**：封装与解封装（文件/网络流的读写、分流）
- **libavcodec**：音视频编解码（压缩/解压缩）
- **libswscale**：视频像素格式和尺寸转换
- **libswresample**：音频采样率和声道转换
- **libavutil**：通用工具库（数据结构、数学、时间等）

### 2.2 FFmpeg CLI 分步处理 trump.mp4

#### 1. 查看文件流信息

```sh
ffprobe trump.mp4
```
显示所有流（视频、音频、字幕等）的详细参数。

#### 2. 分流（解复用）

将 trump.mp4 的视频流和音频流分别提取出来，不做转码：

```sh
ffmpeg -i trump.mp4 -c copy -map 0:v:0 video.h264 -map 0:a:0 audio.aac
```
- `-c copy`：直接复制流，不转码
- `-map 0:v:0`：选择第一个视频流
- `-map 0:a:0`：选择第一个音频流

#### 3. 解码为原始数据

将视频流解码为 YUV 原始数据，音频流解码为 PCM 原始数据：

```sh
# 视频解码为 YUV
ffmpeg -i trump.mp4 -an -c:v rawvideo -pix_fmt yuv420p video.yuv

# 音频解码为 PCM
ffmpeg -i trump.mp4 -vn -c:a pcm_s16le -ar 44100 -ac 2 audio.pcm
```
- `-an`：不处理音频
- `-vn`：不处理视频
- `-c:v rawvideo`：视频解码为原始帧
- `-pix_fmt yuv420p`：指定 YUV 格式
- `-c:a pcm_s16le`：音频解码为 16 位 PCM
- `-ar`、`-ac`：采样率和声道数


> 解码后的 YUV/PCM 数据可通过 OpenGL/SDL2 等工具进行渲染和播放。实际开发中，FFmpeg 的库函数可用于自动完成分流、解码、格式转换等步骤，最终将数据送入渲染流水线。



## 3. OpenGL

OpenGL 是跨平台的图形渲染 API，常用于视频渲染、图像处理等场景。（渲染：将 YUV 数据转换为 RGB 并渲染到屏幕上。）

### 3.1 关键头文件

```c
#include <GL/gl.h>        // OpenGL 核心 API
#include <GL/glu.h>       // OpenGL 实用工具库
#include <GL/glext.h>     // OpenGL 扩展
#include <GLFW/glfw3.h>   // 跨平台窗口和输入（可选）
```

### 3.2 常用函数 API

#### 1. 初始化与窗口管理
```c
glfwInit();               // 初始化 GLFW
glfwCreateWindow();       // 创建窗口
glfwMakeContextCurrent(); // 绑定 OpenGL 上下文
```

#### 2. 纹理相关
```c
glGenTextures();          // 创建纹理对象
glBindTexture();          // 绑定纹理
glTexImage2D();           // 上传像素数据到纹理（如 YUV 分量）
glTexParameteri();        // 设置纹理参数（如过滤方式）
```

#### 3. 着色器相关
```c
glCreateShader();         // 创建着色器对象
glShaderSource();         // 设置着色器源码
glCompileShader();        // 编译着色器
glCreateProgram();        // 创建着色器程序
glAttachShader();         // 绑定着色器到程序
glLinkProgram();          // 链接着色器程序
glUseProgram();           // 使用着色器程序
glGetUniformLocation();   // 获取 uniform 变量位置
glUniform1i();            // 设置 uniform 变量（如纹理单元）
```

#### 4. 顶点与绘制
```c
glGenBuffers();           // 创建缓冲区对象
glBindBuffer();           // 绑定缓冲区
glBufferData();           // 上传顶点数据
glEnableVertexAttribArray(); // 启用顶点属性
glVertexAttribPointer();  // 设置顶点属性指针
glDrawArrays();           // 绘制顶点（如渲染一帧视频）
```

#### 5. FBO（帧缓冲对象）
```c
glGenFramebuffers();      // 创建 FBO
glBindFramebuffer();      // 绑定 FBO
glFramebufferTexture2D(); // 将纹理附加到 FBO
```

### 3.3 常见流程简述

1. 初始化窗口和 OpenGL 上下文
2. 创建并上传 YUV 数据到纹理
3. 编写并编译 YUV->RGB 转换着色器（Shader）
4. 设置顶点数据，绘制到屏幕
5. 可选：使用 FBO 做离屏渲染或后处理

> OpenGL 配合 SDL2 可实现音视频同步播放和渲染。

## 4. SDL2

SDL2（Simple DirectMedia Layer 2）是跨平台的多媒体开发库，常用于音视频播放、窗口管理和输入处理。它在音视频播放器开发中非常常见，尤其适合与 FFmpeg、OpenGL 配合使用。

### 4.1 SDL 音频回调 / SDL 音频设备（拉模式 / 推模式）

- **拉模式（Pull）**：SDL 定期调用用户注册的回调函数，主动“拉取”音频数据填充缓冲区。
- **推模式（Push）**：应用主动将音频数据“推送”到 SDL 的音频缓冲区。
- 常用 API：
  ```c
  SDL_OpenAudioDevice();      // 打开音频设备
  SDL_PauseAudioDevice();     // 控制音频播放/暂停
  SDL_QueueAudio();           // 推送音频数据（推模式）
  SDL_AudioCallback;          // 用户自定义音频回调（拉模式）
  ```

### 4.2 音频参数匹配与重采样

- 音频播放前需确保采样率、声道数、格式与设备一致，否则需重采样（如用 FFmpeg 的 swresample）。
- SDL2 支持多种音频格式，但通常推荐使用 PCM 格式（如 S16LE）。
- 常用 API：
  ```c
  SDL_AudioSpec;              // 音频参数结构体
  SDL_BuildAudioCVT();        // 构建音频转换器
  SDL_ConvertAudio();         // 执行音频重采样和格式转换
  ```

### 4.3 视频渲染（SDL_Renderer）

- SDL2 提供窗口和渲染器，可用于显示视频帧（如 YUV/RGB）。
- 常用 API：
  ```c
  SDL_CreateWindow();         // 创建窗口
  SDL_CreateRenderer();       // 创建渲染器
  SDL_CreateTexture();        // 创建纹理（存储视频帧）
  SDL_UpdateTexture();        // 更新纹理数据（如 YUV 分量）
  SDL_RenderCopy();           // 渲染纹理到窗口
  SDL_RenderPresent();        // 显示渲染结果
  ```

### 4.4 同步（Audio/Video Sync）

- 播放器需保证音视频同步，常用方法有：
  - 以音频为主时钟，视频帧根据音频时间戳显示
  - 计算 AV 差值，动态调整视频帧显示或音频播放速度
- SDL2 提供定时器和事件机制辅助同步：
  ```c
  SDL_GetTicks();             // 获取当前时间（毫秒）
  SDL_Delay();                // 延时
  ```

> SDL2 是音视频播放器开发的基础工具，负责窗口、音频、视频渲染和同步，配合 FFmpeg 可实现完整的解码与播放流程。
